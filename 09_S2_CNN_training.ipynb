{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7cc2421-f9fd-405c-97e7-b3f52787d58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(PCG64) at 0x222CE648BA0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time \n",
    "import os \n",
    "import geopandas as gpd \n",
    "import rioxarray\n",
    "from msmla50 import MSMLA50\n",
    "import glob\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import v2 as transforms\n",
    "import cnn_utils\n",
    "import utils\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "random.seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "np.random.default_rng(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18223bdb-d7ef-4cfd-80c8-6a2057126e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "patch_size = 32\n",
    "stride = 10\n",
    "gt_stride = 32\n",
    "background_label = 0\n",
    "batch_size = 128\n",
    "offset_left = 'best'\n",
    "offset_top = 'best'\n",
    "\n",
    "alpha=0.5\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=90),\n",
    "])\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2154bbc-e34f-4168-9c0c-419b99ea70d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## inputs\n",
    "# berlin\n",
    "image = r'imagery\\berlin_20170519.tif'\n",
    "splited_ref_data = gpd.read_file(r'ref_data\\berlin_ref_splitS2S3S4.gpkg')\n",
    "location = 'berlin'\n",
    "\n",
    "## hong kong\n",
    "# image = r'imagery\\hongkong_20180321.tif'\n",
    "# splited_ref_data = gpd.read_file(r'ref_data\\hongkong_ref_splitS2S3S4.gpkg')\n",
    "# location = 'hongkong'\n",
    "\n",
    "## paris\n",
    "# image = r'imagery\\paris_20170526.tif'\n",
    "# splited_ref_data = gpd.read_file(r'ref_data\\splited_S234\\paris_ref_splitS2S3S4.gpkg')\n",
    "# location = 'paris'\n",
    "\n",
    "## rome\n",
    "# image = r'imagery\\rome_20170620.tif'\n",
    "# splited_ref_data = gpd.read_file(r'ref_data\\splited_S234\\rome_ref_splitS2S3S4.gpkg')\n",
    "# location = 'rome'\n",
    "\n",
    "## sao paulo\n",
    "# image = r'imagery\\sao_paulo_20170726.tif'\n",
    "# splited_ref_data = gpd.read_file(r'ref_data\\splited_S234\\saopaulo_ref_splitS2S3S4.gpkg)\n",
    "# location = 'saopaulo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3e483f-6dcd-40c0-99ce-9e8ab575b926",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39515671-964b-428b-9f1f-79801b26566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = [0,1,2,3,4,5]\n",
    "for fold in folds:\n",
    "    ## prepare train and test polygons\n",
    "    test_polygons = splited_ref_data[splited_ref_data[\"fold\"] == fold]\n",
    "    train_polygons = splited_ref_data[splited_ref_data[\"fold\"] != fold]\n",
    "\n",
    "    train_polygons_raster = fr\"{location}_train_f{fold}.tif\"\n",
    "    test_polygons_raster = fr\"{location}_test_f{fold}.tif\"\n",
    "\n",
    "    # rasterize\n",
    "    train_temp = train_polygons_raster.replace(\".tif\", \"_temp.tif\")\n",
    "    test_temp = test_polygons_raster.replace(\".tif\", \"_temp.tif\")\n",
    "    utils.rasterize_reference_polygons(train_polygons, image, train_temp)\n",
    "    utils.rasterize_reference_polygons(test_polygons, image, test_temp)\n",
    "\n",
    "    # train and test images matched to 10m image\n",
    "    train_image_matched = utils.match_rasters(train_temp, image)\n",
    "    test_image_matched = utils.match_rasters(test_temp, image)\n",
    "    \n",
    "    # save\n",
    "    train_image_matched.rio.to_raster(train_polygons_raster, driver=\"GTiff\", compress=\"LZW\")\n",
    "    test_image_matched.rio.to_raster(test_polygons_raster, driver=\"GTiff\", compress=\"LZW\")\n",
    "\n",
    "    # cleaning\n",
    "    train_image_matched.close()\n",
    "    test_image_matched.close()\n",
    "    train_image_matched = None\n",
    "    test_image_matched = None\n",
    "    gc.collect()\n",
    "    if os.path.exists(train_temp):\n",
    "        os.remove(train_temp)\n",
    "    if os.path.exists(test_temp):\n",
    "        os.remove(test_temp)\n",
    "\n",
    "    ## get train and test patches\n",
    "    train_patches = cnn_utils.generate_labeled_patches_loader(image_path = image,reference_path = train_polygons_raster,patch_size = patch_size,stride = gt_stride,batch_size = batch_size,offset_left = offset_left,offset_top = offset_top,background_label = background_label)\n",
    "    test_patches = cnn_utils.generate_labeled_patches_loader(image_path = image,reference_path = test_polygons_raster,patch_size = patch_size,stride = gt_stride,batch_size = batch_size,offset_left = offset_left,offset_top = offset_top,background_label = background_label)\n",
    "\n",
    "    ## remapping labels\n",
    "    label_mapping = cnn_utils.compute_label_mapping(train_patches)\n",
    "    train_patches = cnn_utils.label_remapping(train_patches, label_mapping)\n",
    "    test_patches = cnn_utils.label_remapping(test_patches, label_mapping)\n",
    "\n",
    "    ## normalize, augment, over/under-sample\n",
    "    mean, std = cnn_utils.get_normalization_parameters(train_patches)\n",
    "    train_patches_norm = cnn_utils.generate_augmented_loader_with_sampler(\n",
    "        image_path=image,\n",
    "        reference_path=train_polygons_raster,\n",
    "        patch_size=patch_size,\n",
    "        stride=gt_stride,\n",
    "        batch_size=batch_size,\n",
    "        offset_left=offset_left,\n",
    "        offset_top=offset_top,\n",
    "        background_label=background_label,\n",
    "        transform=train_transform,\n",
    "        label_mapping=label_mapping,\n",
    "        mean=mean,\n",
    "        std=std,\n",
    "        alpha=alpha\n",
    "    )\n",
    "    test_patches_norm = cnn_utils.normalize_loader(test_patches, mean, std)\n",
    "\n",
    "    ## initalize model\n",
    "    model = MSMLA50(input_channels=10, depth=[16,32,48], num_classes=len(train_polygons[\"gridcode\"].unique()))\n",
    "    model = model.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    output = fr's2_cnn_models\\{location}_S2_fold{fold}.pth'\n",
    "    cnn_utils.cnn_training(model, train_patches_norm, test_patches_norm, num_epochs, criterion, learning_rate, optimizer, output)\n",
    "\n",
    "    ## keep the best model out of the checkpointed models\n",
    "    search_pattern = fr's2_cnn_models\\{location}_S2_fold{fold}_epoch*.pth'\n",
    "    checkpoint_files = glob.glob(search_pattern)\n",
    "    if not checkpoint_files:\n",
    "        print(f\"No checkpoints found for Fold {fold} in {location}\")\n",
    "    else:\n",
    "        best_file = None\n",
    "        best_score = -float('inf') \n",
    "        print(f\"Found {len(checkpoint_files)} checkpoints.\")\n",
    "\n",
    "    for f_path in checkpoint_files:\n",
    "        checkpoint = torch.load(f_path, map_location='cpu')\n",
    "        # searching for highest test accuracy\n",
    "        current_score = checkpoint['test_accuracy']\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_file = f_path\n",
    "\n",
    "    print(f\"Fold {fold}: {os.path.basename(best_file)} (Score: {best_score:.4f})\")\n",
    "\n",
    "    # delete the others\n",
    "    for f_path in checkpoint_files:\n",
    "        if f_path != best_file:\n",
    "            try:\n",
    "                os.remove(f_path)\n",
    "            except OSError as e:\n",
    "                pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
