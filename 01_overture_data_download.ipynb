{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250b7bd2-6518-4c67-a9af-7e58f5f62008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "\n",
    "import io\n",
    "import json\n",
    "from typing import List, Optional, Tuple\n",
    "from urllib.request import urlopen\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.fs as fs\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc140d4-2b6e-4f38-84bb-172db9bd7471",
   "metadata": {},
   "source": [
    "## Overture Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db4d8c7-1032-4dbe-90e0-833927114b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @overturemaps-py, core.py\n",
    "# https://github.com/OvertureMaps/overturemaps-py/blob/main/overturemaps/core.py\n",
    "\n",
    "STAC_CATALOG_URL = \"https://stac.overturemaps.org/catalog.json\"\n",
    "\n",
    "# Cache for STAC catalog to avoid repeated network calls\n",
    "_cached_stac_catalog = None\n",
    "\n",
    "# Allows for optional import of additional dependencies\n",
    "try:\n",
    "    from geopandas import GeoDataFrame\n",
    "\n",
    "    HAS_GEOPANDAS = True\n",
    "except ImportError:\n",
    "    HAS_GEOPANDAS = False\n",
    "    class GeoDataFrame: pass\n",
    "\n",
    "\n",
    "def _get_stac_catalog() -> dict:\n",
    "    \"\"\"\n",
    "    Fetch and cache the STAC catalog.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict: The STAC catalog JSON\n",
    "    \"\"\"\n",
    "    global _cached_stac_catalog\n",
    "\n",
    "    if _cached_stac_catalog is not None:\n",
    "        return _cached_stac_catalog\n",
    "\n",
    "    try:\n",
    "        with urlopen(STAC_CATALOG_URL) as response:\n",
    "            catalog = json.load(response)\n",
    "\n",
    "        # Cache the catalog\n",
    "        _cached_stac_catalog = catalog\n",
    "        return catalog\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Could not fetch STAC catalog: {e}\") from e\n",
    "\n",
    "\n",
    "def get_available_releases() -> Tuple[List[str], str]:\n",
    "    \"\"\"\n",
    "    Fetch available releases from the STAC catalog.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple of (all_releases, latest_release) where:\n",
    "        - all_releases is a list of release version strings\n",
    "        - latest_release is the latest release version string\n",
    "    \"\"\"\n",
    "    catalog = _get_stac_catalog()\n",
    "\n",
    "    latest_release = catalog.get(\"latest\")\n",
    "\n",
    "    # Extract release versions from the child links\n",
    "    releases = []\n",
    "    for link in catalog.get(\"links\", []):\n",
    "        if link.get(\"rel\") == \"child\":\n",
    "            href = link.get(\"href\", \"\")\n",
    "            # href format is \"./2025-09-24.0/catalog.json\"\n",
    "            release_version = href.strip(\"./\").split(\"/\")[0]\n",
    "            if release_version:\n",
    "                releases.append(release_version)\n",
    "\n",
    "    return releases, latest_release\n",
    "\n",
    "\n",
    "def get_latest_release() -> str:\n",
    "    \"\"\"\n",
    "    Get the latest release version.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str: The latest release version\n",
    "    \"\"\"\n",
    "    _, latest = get_available_releases()\n",
    "    return latest\n",
    "\n",
    "\n",
    "# For backwards compatibility, expose ALL_RELEASES as a list\n",
    "# This will be populated dynamically when first accessed\n",
    "def _get_all_releases():\n",
    "    releases, _ = get_available_releases()\n",
    "    return releases\n",
    "\n",
    "\n",
    "# Lazy evaluation property-like access\n",
    "class _ReleasesProxy:\n",
    "    def __getitem__(self, index):\n",
    "        return _get_all_releases()[index]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(_get_all_releases())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(_get_all_releases())\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(_get_all_releases())\n",
    "\n",
    "\n",
    "ALL_RELEASES = _ReleasesProxy()\n",
    "\n",
    "\n",
    "def _get_files_from_stac(\n",
    "    theme: str, overture_type: str, bbox: tuple, release: str\n",
    ") -> Optional[List[str]]:\n",
    "    \"\"\"\n",
    "    Returns a list of bucket/key paths using the STAC-geoparquet index\n",
    "    \"\"\"\n",
    "    stac_url = f\"https://stac.overturemaps.org/{release}/collections.parquet\"\n",
    "    try:\n",
    "        # Arrow can't read HTTP URLs directly; read into memory first\n",
    "        with urlopen(stac_url) as response:\n",
    "            data = response.read()\n",
    "            buffer = io.BytesIO(data)\n",
    "            stac_table = pq.read_table(buffer)\n",
    "\n",
    "        feature_type_filter = (pc.field(\"collection\") == overture_type) & (\n",
    "            pc.field(\"type\") == \"Feature\"\n",
    "        )\n",
    "\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        bbox_filter = (\n",
    "            (pc.field(\"bbox\", \"xmin\") < xmax)\n",
    "            & (pc.field(\"bbox\", \"xmax\") > xmin)\n",
    "            & (pc.field(\"bbox\", \"ymin\") < ymax)\n",
    "            & (pc.field(\"bbox\", \"ymax\") > ymin)\n",
    "        )\n",
    "\n",
    "        combined_filter = feature_type_filter & bbox_filter\n",
    "        table = stac_table.filter(combined_filter)\n",
    "\n",
    "        if table.num_rows > 0:\n",
    "            file_paths = table.column(\"assets\").to_pylist()\n",
    "\n",
    "            # clip out the \"s3://\" prefix\n",
    "            s3_paths = [path[\"aws-s3\"][\"href\"][len(\"s3://\") :] for path in file_paths]\n",
    "            return s3_paths\n",
    "        else:\n",
    "            print(f\"No data found for release {release} in query bbox {bbox}.\")\n",
    "            return []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading STAC index at {stac_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def _create_s3_record_batch_reader(\n",
    "    path,\n",
    "    filter_expr=None,\n",
    "    connect_timeout=None,\n",
    "    request_timeout=None,\n",
    ") -> Optional[pa.RecordBatchReader]:\n",
    "    \"\"\"\n",
    "    Create a RecordBatchReader from S3 path(s) with optional filtering.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str or list of str\n",
    "        S3 path(s) in the format \"bucket/key\" (without s3:// prefix)\n",
    "    filter_expr: pyarrow expression, optional\n",
    "        Filter to apply when reading the dataset\n",
    "    connect_timeout: int, optional\n",
    "        Connection timeout in seconds\n",
    "    request_timeout: int, optional\n",
    "        Request timeout in seconds\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    RecordBatchReader with the feature data, or None if error occurs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataset = ds.dataset(\n",
    "            path,\n",
    "            filesystem=fs.S3FileSystem(\n",
    "                anonymous=True,\n",
    "                region=\"us-west-2\",\n",
    "                connect_timeout=connect_timeout,\n",
    "                request_timeout=request_timeout,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        batches = dataset.to_batches(filter=filter_expr)\n",
    "\n",
    "        # Filter out empty batches to avoid downstream issues\n",
    "        non_empty_batches = (b for b in batches if b.num_rows > 0)\n",
    "\n",
    "        geoarrow_schema = geoarrow_schema_adapter(dataset.schema)\n",
    "        reader = pa.RecordBatchReader.from_batches(geoarrow_schema, non_empty_batches)\n",
    "\n",
    "        return reader\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading data from path {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def record_batch_reader(\n",
    "    overture_type,\n",
    "    bbox=None,\n",
    "    release=None,\n",
    "    connect_timeout=None,\n",
    "    request_timeout=None,\n",
    "    stac=False,\n",
    ") -> Optional[pa.RecordBatchReader]:\n",
    "    \"\"\"\n",
    "    Return a pyarrow RecordBatchReader for the desired bounding box and s3 path\n",
    "    \"\"\"\n",
    "\n",
    "    if release is None:\n",
    "        release = get_latest_release()\n",
    "    path = _dataset_path(overture_type, release)\n",
    "\n",
    "    intersecting_files = None\n",
    "    if bbox and stac:\n",
    "        intersecting_files = _get_files_from_stac(\n",
    "            type_theme_map[overture_type], overture_type, bbox, release\n",
    "        )\n",
    "\n",
    "    if bbox:\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        filter_expr = (\n",
    "            (pc.field(\"bbox\", \"xmin\") < xmax)\n",
    "            & (pc.field(\"bbox\", \"xmax\") > xmin)\n",
    "            & (pc.field(\"bbox\", \"ymin\") < ymax)\n",
    "            & (pc.field(\"bbox\", \"ymax\") > ymin)\n",
    "        )\n",
    "    else:\n",
    "        filter_expr = None\n",
    "\n",
    "    return _create_s3_record_batch_reader(\n",
    "        intersecting_files if intersecting_files else path,\n",
    "        filter_expr=filter_expr,\n",
    "        connect_timeout=connect_timeout,\n",
    "        request_timeout=request_timeout,\n",
    "    )\n",
    "\n",
    "\n",
    "def geodataframe(\n",
    "    overture_type: str,\n",
    "    bbox: tuple[float, float, float, float] = None,\n",
    "    release: str = None,\n",
    "    connect_timeout: int = None,\n",
    "    request_timeout: int = None,\n",
    "    stac: bool = False\n",
    ") -> GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Loads geoparquet for specified type into a geopandas dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    overture_type: type to load\n",
    "    bbox: optional bounding box for data fetch (xmin, ymin, xmax, ymax)\n",
    "    connect_timeout: optional connection timeout in seconds\n",
    "    request_timeout: optional request timeout in seconds\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GeoDataFrame with the optionally filtered theme data\n",
    "\n",
    "    \"\"\"\n",
    "    if not HAS_GEOPANDAS:\n",
    "        raise ImportError(\"geopandas is required to use this function\")\n",
    "\n",
    "    reader = record_batch_reader(\n",
    "        overture_type,\n",
    "        bbox=bbox,\n",
    "        release=release,\n",
    "        connect_timeout=connect_timeout,\n",
    "        request_timeout=request_timeout,\n",
    "        stac=stac\n",
    "    )\n",
    "    return GeoDataFrame.from_arrow(reader)\n",
    "\n",
    "\n",
    "def geoarrow_schema_adapter(schema: pa.Schema) -> pa.Schema:\n",
    "    \"\"\"\n",
    "    Convert a geoarrow-compatible schema to a proper geoarrow schema\n",
    "\n",
    "    This assumes there is a single \"geometry\" column with WKB formatting\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    schema: pa.Schema\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pa.Schema\n",
    "    A copy of the input schema with the geometry field replaced with\n",
    "    a new one with the proper geoarrow ARROW:extension metadata\n",
    "\n",
    "    \"\"\"\n",
    "    geometry_field_index = schema.get_field_index(\"geometry\")\n",
    "    geometry_field = schema.field(geometry_field_index)\n",
    "    geoarrow_geometry_field = geometry_field.with_metadata(\n",
    "        {b\"ARROW:extension:name\": b\"geoarrow.wkb\"}\n",
    "    )\n",
    "\n",
    "    geoarrow_schema = schema.set(geometry_field_index, geoarrow_geometry_field)\n",
    "\n",
    "    return geoarrow_schema\n",
    "\n",
    "\n",
    "type_theme_map = {\n",
    "    \"address\": \"addresses\",\n",
    "    \"bathymetry\": \"base\",\n",
    "    \"building\": \"buildings\",\n",
    "    \"building_part\": \"buildings\",\n",
    "    \"division\": \"divisions\",\n",
    "    \"division_area\": \"divisions\",\n",
    "    \"division_boundary\": \"divisions\",\n",
    "    \"place\": \"places\",\n",
    "    \"segment\": \"transportation\",\n",
    "    \"connector\": \"transportation\",\n",
    "    \"infrastructure\": \"base\",\n",
    "    \"land\": \"base\",\n",
    "    \"land_cover\": \"base\",\n",
    "    \"land_use\": \"base\",\n",
    "    \"water\": \"base\",\n",
    "}\n",
    "\n",
    "\n",
    "def _dataset_path(overture_type: str, release: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the s3 path of the Overture dataset to use. This assumes overture_type has\n",
    "    been validated, e.g. by the CLI\n",
    "\n",
    "    \"\"\"\n",
    "    # Map of sub-partition \"type\" to parent partition \"theme\" for forming the\n",
    "    # complete s3 path. Could be discovered by reading from the top-level s3\n",
    "    # location but this allows to only read the files in the necessary partition.\n",
    "    theme = type_theme_map[overture_type]\n",
    "    return (\n",
    "        f\"overturemaps-us-west-2/release/{release}/theme={theme}/type={overture_type}/\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_all_overture_types() -> List[str]:\n",
    "    return list(type_theme_map.keys())\n",
    "\n",
    "\n",
    "# Registry manifest is now part of the STAC catalog\n",
    "# Access via catalog.json -> registry property -> manifest field\n",
    "\n",
    "\n",
    "def _binary_search_manifest(\n",
    "    manifest_tuples: List[Tuple[str, str]], gers_id: str\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Binary search through manifest tuples to find the file containing the given GERS ID.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    manifest_tuples: List of (filename, max_id) tuples, sorted by max_id\n",
    "    gers_id: The GERS ID to search for (lowercase)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Filename containing the ID, or None if not found\n",
    "    \"\"\"\n",
    "    left, right = 0, len(manifest_tuples) - 1\n",
    "\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        filename, max_id = manifest_tuples[mid]\n",
    "\n",
    "        if gers_id <= max_id:\n",
    "            # Check if this is the first file where max_id >= gers_id\n",
    "            if mid == 0 or manifest_tuples[mid - 1][1] < gers_id:\n",
    "                return filename\n",
    "            else:\n",
    "                # Search in the left half\n",
    "                right = mid - 1\n",
    "        else:\n",
    "            # Search in the right half\n",
    "            left = mid + 1\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def query_gers_registry(gers_id: str) -> Optional[Tuple[str, List[float]]]:\n",
    "    \"\"\"\n",
    "    Query the GERS registry to get the filepath and bbox for a given GERS ID.\n",
    "\n",
    "    The registry always uses the latest release.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gers_id: The GERS ID to look up\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple of (filepath, bbox) where bbox is [xmin, ymin, xmax, ymax], or None if not found\n",
    "    \"\"\"\n",
    "    import sys\n",
    "\n",
    "    release = get_latest_release()\n",
    "    release_path = f\"overturemaps-us-west-2/release/{release}\"\n",
    "    gers_id_lower = gers_id.lower()\n",
    "\n",
    "    try:\n",
    "        # Get the cached STAC catalog\n",
    "        catalog = _get_stac_catalog()\n",
    "\n",
    "        # Get the registry object from the catalog\n",
    "        registry = catalog.get(\"registry\")\n",
    "        if registry is None:\n",
    "            print(\"Registry configuration not found in STAC catalog\", file=sys.stderr)\n",
    "            return None\n",
    "\n",
    "        # The registry contains 'path' and 'manifest'\n",
    "        # manifest is a list of [filename, max_id] tuples\n",
    "        registry_path = registry.get(\"path\", \"\")\n",
    "        manifest_tuples = registry.get(\"manifest\", [])\n",
    "\n",
    "        if not manifest_tuples:\n",
    "            print(\"Registry manifest is empty in STAC catalog\", file=sys.stderr)\n",
    "            return None\n",
    "\n",
    "        # Use binary search to find the file containing this GERS ID\n",
    "        registry_file = _binary_search_manifest(manifest_tuples, gers_id_lower)\n",
    "\n",
    "        if registry_file is None:\n",
    "            print(f\"{gers_id} does not exist in the GERS Registry.\", file=sys.stderr)\n",
    "            return None\n",
    "\n",
    "        # Read the specific registry file with filter (predicate pushdown)\n",
    "        # This only reads the relevant row groups instead of the entire file\n",
    "        registry_path = f\"overturemaps-us-west-2/registry/{registry_file}\"\n",
    "        filesystem = fs.S3FileSystem(anonymous=True, region=\"us-west-2\")\n",
    "\n",
    "        # Use filters parameter for predicate pushdown\n",
    "        filtered_table = pq.read_table(\n",
    "            registry_path, filesystem=filesystem, filters=[(\"id\", \"=\", gers_id_lower)]\n",
    "        )\n",
    "\n",
    "        if filtered_table.num_rows == 0:\n",
    "            print(f\"{gers_id} does not exist in the GERS Registry.\", file=sys.stderr)\n",
    "            return None\n",
    "\n",
    "        # Get the first (should be only) result\n",
    "        row = filtered_table.to_pylist()[0]\n",
    "        path = row[\"path\"]\n",
    "        bbox_struct = row.get(\"bbox\")\n",
    "        version = row.get(\"version\")\n",
    "        first_seen = row.get(\"first_seen\")\n",
    "        last_seen = row.get(\"last_seen\")\n",
    "        last_changed = row.get(\"last_changed\")\n",
    "\n",
    "        # Check if path is NULL - means feature is not present in current release\n",
    "        if path is None:\n",
    "            print(\n",
    "                f\"GERS ID '{gers_id}' found in registry but not present in release {release}\",\n",
    "                file=sys.stderr,\n",
    "            )\n",
    "            print(f\"  Version: {version}\", file=sys.stderr)\n",
    "            print(f\"  First seen: {first_seen}\", file=sys.stderr)\n",
    "            print(f\"  Last seen: {last_seen}\", file=sys.stderr)\n",
    "            if last_changed:\n",
    "                print(f\"  Last changed: {last_changed}\", file=sys.stderr)\n",
    "            return None\n",
    "\n",
    "        # Construct full filepath\n",
    "        if not path.startswith(\"/\"):\n",
    "            path = \"/\" + path\n",
    "        filepath = f\"{release_path}{path}\"\n",
    "\n",
    "        # Extract bbox values if available\n",
    "        if bbox_struct is not None:\n",
    "            bbox = [\n",
    "                bbox_struct[\"xmin\"],\n",
    "                bbox_struct[\"ymin\"],\n",
    "                bbox_struct[\"xmax\"],\n",
    "                bbox_struct[\"ymax\"],\n",
    "            ]\n",
    "        else:\n",
    "            bbox = None\n",
    "\n",
    "        # Write registry information to stderr\n",
    "        print(f\"Found GERS ID '{gers_id}' in release {release}\", file=sys.stderr)\n",
    "        print(f\"  Version: {version}\", file=sys.stderr)\n",
    "        print(f\"  Filepath: s3://{filepath}\", file=sys.stderr)\n",
    "        if bbox is not None:\n",
    "            print(\n",
    "                f\"  Bbox: [{bbox[0]:.6f}, {bbox[1]:.6f}, {bbox[2]:.6f}, {bbox[3]:.6f}]\",\n",
    "                file=sys.stderr,\n",
    "            )\n",
    "        else:\n",
    "            print(f\"  Bbox: None\", file=sys.stderr)\n",
    "        print(f\"  First seen: {first_seen}\", file=sys.stderr)\n",
    "        print(f\"  Last seen: {last_seen}\", file=sys.stderr)\n",
    "        if last_changed:\n",
    "            print(f\"  Last changed: {last_changed}\", file=sys.stderr)\n",
    "\n",
    "        return (filepath, bbox)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying GERS registry: {e}\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "\n",
    "def record_batch_reader_from_gers(\n",
    "    gers_id: str,\n",
    "    connect_timeout: int = None,\n",
    "    request_timeout: int = None,\n",
    "    registry_result: Optional[Tuple[str, List[float]]] = None,\n",
    ") -> Optional[pa.RecordBatchReader]:\n",
    "    \"\"\"\n",
    "    Return a pyarrow RecordBatchReader for a specific GERS ID.\n",
    "\n",
    "    The registry always uses the latest release.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gers_id: The GERS ID to look up\n",
    "    connect_timeout: Optional connection timeout in seconds\n",
    "    request_timeout: Optional request timeout in seconds\n",
    "    registry_result: Optional pre-fetched registry result (filepath, bbox)\n",
    "                    to avoid duplicate registry queries\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    RecordBatchReader with the feature data, or None if not found\n",
    "    \"\"\"\n",
    "    # Use pre-fetched result if provided, otherwise query the registry\n",
    "    if registry_result is None:\n",
    "        result = query_gers_registry(gers_id)\n",
    "        if result is None:\n",
    "            return None\n",
    "    else:\n",
    "        result = registry_result\n",
    "\n",
    "    filepath, bbox = result\n",
    "\n",
    "    # Build filter expression based on ID and bbox (if available)\n",
    "    filter_expr = pc.field(\"id\") == gers_id.lower()\n",
    "\n",
    "    if bbox is not None:\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        bbox_filter = (\n",
    "            (pc.field(\"bbox\", \"xmin\") == xmin)\n",
    "            & (pc.field(\"bbox\", \"ymin\") == ymin)\n",
    "            & (pc.field(\"bbox\", \"xmax\") == xmax)\n",
    "            & (pc.field(\"bbox\", \"ymax\") == ymax)\n",
    "        )\n",
    "        filter_expr = filter_expr & bbox_filter\n",
    "\n",
    "    return _create_s3_record_batch_reader(\n",
    "        filepath,\n",
    "        filter_expr=filter_expr,\n",
    "        connect_timeout=connect_timeout,\n",
    "        request_timeout=request_timeout,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7864b77-756d-4a03-a495-7e1870c35d03",
   "metadata": {},
   "source": [
    "## Download Overture Data for study sites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df35c34-503e-4fbb-a10a-86244ba9e46d",
   "metadata": {},
   "source": [
    "Download and save Overture Data (buildings, streets, water features) for study site area defined by buffered bounding box of site's LCZ reference dataset.\n",
    "\n",
    "Used release of Overture Data: 2025-02-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04aaf3d-4592-4a3c-8767-7f31344a714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load LCZ reference dataset of a site\n",
    "ref_gdf = gpd.read_file(r\"ref_data\\berlin_ref.gpkg\")\n",
    "#ref_gdf = gpd.read_file(r\"ref_data\\hongkong_ref.gpkg\")\n",
    "#ref_gdf = gpd.read_file(r\"ref_data\\paris_ref.gpkg\")\n",
    "#ref_gdf = gpd.read_file(r\"ref_data\\rome_ref.gpkg\")\n",
    "#ref_gdf = gpd.read_file(r\"ref_data\\saopaulo_ref.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8840b0d-03c5-4137-b685-0adbffa18de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get CRS\n",
    "ref_epsg = ref_gdf.crs.to_epsg()\n",
    "\n",
    "# get bbox\n",
    "minx, miny, maxx, maxy = ref_gdf.total_bounds\n",
    "ref_bbox = box(minx, miny, maxx, maxy)\n",
    "ref_bbox_gdf = gpd.GeoDataFrame(geometry=[ref_bbox], crs=ref_gdf.crs)\n",
    "\n",
    "# buffer bbox by 3 km\n",
    "ref_bbox_buffer = ref_bbox_gdf.buffer(3000)\n",
    "ref_bbox_buffer_gdf = gpd.GeoDataFrame(geometry=ref_bbox_buffer, crs=ref_gdf.crs)\n",
    "\n",
    "# convert to 4326\n",
    "ref_bbox_buffer_gdf = ref_bbox_buffer_gdf.to_crs(epsg=4326)\n",
    "\n",
    "# get the bbox coordinates\n",
    "xmin, ymin, xmax, ymax = ref_bbox_buffer_gdf.total_bounds\n",
    "bbox = (xmin, ymin, xmax, ymax)\n",
    "bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd36590-6ae7-4e70-b394-986c94de19af",
   "metadata": {},
   "source": [
    "### Building footprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19064299-b7bc-4243-8c8a-f0da5c98debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download\n",
    "gdf = geodataframe(overture_type=\"building\", release='2025-02-19.0', bbox=bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becff0bd-80a2-4c2c-bdd1-adfa2faf0f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df86c8c-a48e-4392-ae81-e3f6dca5cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning\n",
    "buildings = gdf[['geometry']]\n",
    "buildings.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb50b572-412d-4a40-a346-4288e7552b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproject\n",
    "buildings = buildings.set_crs(epsg=4326)\n",
    "buildings = buildings.to_crs(epsg=ref_epsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e9dfcd-7c18-4176-a616-9857243cbfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "buildings.to_file(r\"overture_data\\berlin_buildings.gpkg\", driver=\"GPKG\")\n",
    "#buildings.to_file(r\"overture_data\\hongkong_buildings.gpkg\", driver=\"GPKG\")\n",
    "#buildings.to_file(r\"overture_data\\paris_buildings.gpkg\", driver=\"GPKG\")\n",
    "#buildings.to_file(r\"overture_data\\rome_buildings.gpkg\", driver=\"GPKG\")\n",
    "#buildings.to_file(r\"overture_data\\saopaulo_buildings.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4311c1-6535-40d6-b0f3-425ed846c581",
   "metadata": {},
   "source": [
    "### Street network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4618c733-80bd-4c03-86f8-44cb61d4dbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download\n",
    "gdf = geodataframe(overture_type=\"segment\", release='2025-02-19.0', bbox=bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f5443e-8e68-441d-869f-b2f050b43b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reprojection\n",
    "gdf = gdf.set_crs(epsg=4326)\n",
    "gdf = gdf.to_crs(epsg=ref_epsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8c8e34-472c-494f-bbcf-86ba66640a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract roads\n",
    "streets = gdf[gdf['subtype']=='road']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fea6fb-0341-4a51-9d25-dc59b0796777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# service roads removed \n",
    "approved_roads = ['living_street',\n",
    "                     'motorway',\n",
    "                     'motorway_link',\n",
    "                     'pedestrian',\n",
    "                     'primary',\n",
    "                     'primary_link',\n",
    "                     'residential',\n",
    "                     'secondary',\n",
    "                     'secondary_link',\n",
    "                     'tertiary',\n",
    "                     'tertiary_link',\n",
    "                     'trunk',\n",
    "                     'trunk_link',\n",
    "                     'unclassified']\n",
    "\n",
    "streets = streets[streets['class'].isin(approved_roads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69caaafe-3d3f-4e25-81ba-b0d3cd0283eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop tunnels\n",
    "def to_drop_tunnel(row):\n",
    "    '''Find whether or not a road segment has a tunnel thats more than 50 metres.'''\n",
    "    tunnel_length = row.geometry.length\n",
    "    flags = row.road_flags\n",
    "\n",
    "    total_tunnel_proportion = -1\n",
    "    for flag in flags:\n",
    "        if 'values' in flag and ('is_tunnel' in flag['values']) :\n",
    "            # between could be missing to show the whole thing is a tunnel\n",
    "            total_tunnel_proportion = 0.0 if total_tunnel_proportion < 0 else total_tunnel_proportion\n",
    "            # between could be None to indicate the whole thing is a tunnel \n",
    "            if ('between' in flag) and (flag['between'] is not None):\n",
    "                s,e = flag['between'][0], flag['between'][1]\n",
    "                total_tunnel_proportion += (e - s)\n",
    "    \n",
    "    if (total_tunnel_proportion*tunnel_length) > 50:\n",
    "        return True\n",
    "    elif total_tunnel_proportion == 0.0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "to_filter = streets.loc[~streets.road_flags.isna(), ]\n",
    "tunnels_to_drop = to_filter.apply(to_drop_tunnel, axis=1)\n",
    "streets = streets.drop(to_filter[tunnels_to_drop].index)\n",
    "\n",
    "streets = streets.sort_values('id')[['id', 'geometry', 'class']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a822dcf-19eb-479e-93e4-6d0b6106f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning\n",
    "streets = streets[['geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d3bc2-c846-4d30-86bc-37b961e912ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "streets.to_file(r\"overture_data\\berlin_streets.gpkg\", driver=\"GPKG\")\n",
    "#streets.to_file(r\"overture_data\\hongkong_streets.gpkg\", driver=\"GPKG\")\n",
    "#streets.to_file(r\"overture_data\\paris_streets.gpkg\", driver=\"GPKG\")\n",
    "#streets.to_file(r\"overture_data\\rome_streets.gpkg\", driver=\"GPKG\")\n",
    "#streets.to_file(r\"overture_data\\saopaulo_streets.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edb290e-c699-46f9-a984-18f05dae1abb",
   "metadata": {},
   "source": [
    "### Water features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172559de-a888-4a49-a488-dd3ae6671096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download water features\n",
    "gdf = geodataframe(overture_type=\"water\", release='2025-02-19.0', bbox=bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76749df6-0819-4d2a-b5bb-1f4e021280e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reprojection\n",
    "gdf = gdf.set_crs(epsg=4326)\n",
    "gdf = gdf.to_crs(epsg=ref_epsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acefc45-b741-4a3f-be6d-cbce8178e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.geom_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61acbf84-d9f5-4892-9f96-c45b8925dc57",
   "metadata": {},
   "source": [
    "#### Water bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f600423-febd-4547-b02f-421044e6c732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get water bodies (polygons)\n",
    "water_polygons = gdf[gdf.geometry.type == \"Polygon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd48822b-a079-4178-bfba-c7f85c804db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude human-made, physical, reservoir, spring and wastewater\n",
    "approved_waterbodies = ['water','lake','pond','stream','canal','lake','river','ocean']\n",
    "\n",
    "water_polygons = water_polygons[water_polygons['subtype'].isin(approved_waterbodies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c8d2d-1769-46bd-a3cb-d9802cc4b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude underground and aboveground features \n",
    "water_polygons = water_polygons[(water_polygons['level'].isna()) | (water_polygons['level']==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa69063-6608-407a-9c30-b7564d1050e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "water_polygons.to_file(r\"overture_data\\berlin_waterbodies.gpkg\", driver=\"GPKG\")\n",
    "#water_polygons.to_file(r\"overture_data\\hongkong_waterbodies.gpkg\", driver=\"GPKG\")\n",
    "#water_polygons.to_file(r\"overture_data\\paris_waterbodies.gpkg\", driver=\"GPKG\")\n",
    "#water_polygons.to_file(r\"overture_data\\rome_waterbodies.gpkg\", driver=\"GPKG\")\n",
    "#water_polygons.to_file(r\"overture_data\\saopaulo_waterbodies.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6cb858-ea63-409a-9f75-0bbda1e8c401",
   "metadata": {},
   "source": [
    "#### Waterlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c898be9-1f57-4d2f-b1bc-60f31c3c4b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get water lines\n",
    "water_lines = gdf[gdf.geometry.type == \"LineString\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61d4cae-cbaf-4c1d-843f-160f129cec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extact canals, rivers, streams\n",
    "approved_waterlines = ['canal','river','stream']\n",
    "water_lines = water_lines[water_lines['subtype'].isin(approved_waterlines)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3574824e-c0ad-40c0-92b5-77fde9354aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude underground and aboveground features\n",
    "water_lines = water_lines[(water_lines['level'].isna()) | (water_lines['level']==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b9a7d-bcf0-4a85-a367-5d05c1035c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "water_lines.to_file(r\"overture_data\\berlin_waterlines.gpkg\", driver=\"GPKG\")\n",
    "#water_lines.to_file(r\"overture_data\\hongkong_waterlines.gpkg\", driver=\"GPKG\")\n",
    "#water_lines.to_file(r\"overture_data\\paris_waterlines.gpkg\", driver=\"GPKG\")\n",
    "#water_lines.to_file(r\"overture_data\\rome_waterlines.gpkg\", driver=\"GPKG\")\n",
    "#water_lines.to_file(r\"overture_data\\saopaulo_waterlines.gpkg\", driver=\"GPKG\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
